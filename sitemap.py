'''  sitemap.py 
 
 Python script to process XML data files to produce a sitemap
 
 This script processes an XML file created by a backup of the entire wiki site. Partial 
 backups or category exports can also be processed.

 Scans an exported XML file of the entire site
 1) Generates a site map listing in wiki text form to be displayed on a sitemap page the wiki site.
 2) Optionally, checks for each page that image and media files have been downloaded, and if not, retrieves them from the wiki site.
 3) Optionally generates image description files indicating where each image has been referenced on the site. Note that if 
    running this process more than once, the description files should all be deleted first.
 4) Generates a reference list of pages on the site.
 

 Improvements needed:
 a) Accommodate Profile: pages - treat as biographies and list in same table as Person: pages (done, monitor)
 b) Ignore broken links in the sitemap page (done, monitor)
 c) File list wikitext file with links to file location (pending)
 d) Ability to skip <nowiki>   </nowiki> text sections (skip edit cheat sheet page entirely) (done, monitor)

240828: Moved main pages_file.write outside state loop to ensure main pages only written once!
240828: Code to remove Person: also does Profile:
240828: Lifespan detection and output provided for organisations
240830: Repaired alphabetic tabs for biographies
240930: Repaired sorting for profile pages

Revision 6.5 Integrated with other apps, outputs new pages list
 Only look for lifespan in first 200 characters of page text
 Summaries transferred from reference pages file to new pages file (not yet working)
 Summaries and categories appear with small text in wikitabl (to be evaluated)
 (in progress, not yet working)

'''
import os
import re 
import requests
from urllib.parse import unquote

session = requests.Session()                                    # needed for accessing URLs to download images
outfile = open("sitemap_log.txt","w",encoding="utf-8")          # log file reporting all operations completed

# Specify the folder paths - note that internally Python uses forward slashes, not backslashes as in Windows/MSDOS
folder_path = "C:/D/2024/240315_EHWA/eha"                       # folder containing image files (no slash at end)
download_path = "C:/D/2024/240315_EHWA/eha-downloads/"          # folder for image downloads
description_folder = "C:/D/2024/240315_EHWA/desc/"              # image descriptions folder (with a slash)
wkg_folder = "C:/Users/HP/OneDrive - Close Comfort Pty Ltd/Documents/Python/" # working directory (with slash)
xml_data_file   =  "eha.xml"                                    # xml data file to be processed
page_ref_file_name = "pages_ref.txt"                            # pages reference file
wiki_table_file =  "wiki-table-eha.txt"                         # table of page URLs generated by sitemap.py
new_pages_file_name = "new_pages.txt"                           # page list file generated by sitemap.py
csv_file_name   = "eha_sitemap.xls"                             # spreadsheet for note-keeping
media_file_list_name = "combined_file_list_240516.txt"          # list of files available locally in media folder(s)

# Specify URLs
site_URL = "https://eha.mywikis.wiki/wiki/"                     # base URL for site

# specify the media URL (with a slash at the end)
# click on any image on a wiki page, and the respective media page opens, then right click on the full-size image and choose copy image URL
wiki_url = "https://mywikis-wiki-media.s3.us-central-1.wasabisys.com/eha/"   

categories_file_name = "category_list.txt"                      # categories list

download = False    # set to True for media file downloads and access verification are needed
desc_write = False   # set to True to write description file

# Get a list of all image and media files already in the local media file folder, including PDFs
media_file_list = os.listdir(folder_path)

# Print the number of files in the folder
nfiles = len(media_file_list)
outfile.write("Media folder contains ")
outfile.write(str(nfiles))
outfile.write(" files\n\n")

#=====================================================================================================
#
# function to read list from a file
#
def read_list_file(file_name):
  with open(file_name,"r") as file:
    list = file.read().splitlines()
  file.close()  
  return list 

#=====================================================================================================
#
# function to extract page names
# 
def extract_page_names(text): 
    return re.findall(r'<title>(.*?)</title>', text) 

#=====================================================================================================
#
# functions to extract media files and image files
# 
#  
def extract_image_files(text): 
    image_file_pattern = r'(?<=File:)(.+?)(?=\|)' 
    list = re.findall(image_file_pattern, text) 
    short_list = []
    for file in list:
      if file not in short_list:
        short_list += [file]
    return short_list    
    
# Function to extract media files
def extract_media_files(text):
    media_file_pattern = r'(?<=Media:)(.+?)(?=\|)'
    list = re.findall(media_file_pattern, text) 
    short_list = []
    for file in list:
      if file not in short_list:
        short_list += [file]
    return short_list    


#=====================================================================================================
#
# function to extract birth year and death year from a person's biographical entry
# 
# 
def lifespan(text):
  instance = re.search(r'[\(](\d{4})', text[0:200])  #  Only look in first 200 characters
  if instance:
    instance2 = re.search(r'[\s-]{1,5}([-\d]{4})[\)]', text[instance.end():200])
    if instance2:
      output_text = instance.group(1) + "-" + instance2.group(1)
    else:
      output_text = instance.group(1) + "- ----"
  else:
    output_text = ""
  return  output_text


#=====================================================================================================
#
# function to replace white space and %20 characters in a filename string with underscore characters
# 
# 
def replace_underscore(text):
    text = re.sub(r'&amp;','&', text)
    text = re.sub(r'\s+$', '', text)
    text = re.sub(r'\s', '_', text)
    text = re.sub(r'__','_',text)
    text = text.lstrip('_')
    text = re.sub(r'%20', '_', text)
    text = re.sub(r'%27', '\'', text)   # some files have %27 instead of a single quote character (apostrophe)
    return(text)

#=====================================================================================================
#
# function to convert timestamp format from numeric form to mm-yy
# 
# 
def timestamp_mon_year(text):
    m = text[5:7]
    if m == "01":
      mm = "Jan "
    elif m == "02":
      mm = "Feb "
    elif m == "03":
      mm = "Mar "
    elif m == "04":
      mm = "Apr "
    elif m == "05":
      mm = "May "
    elif m == "06":
      mm = "Jun "
    elif m == "07":
      mm = "Jul "
    elif m == "08":
      mm = "Aug "
    elif m == "09":
      mm = "Sep "
    elif m == "10":
      mm = "Oct "
    elif m == "11":
      mm = "Nov "
    elif m == "12":
      mm = "Dec "
    else:
      mm = "*** "
    return (mm + text[:4])
    
   
#=====================================================================================================
#
# function to to reformat page name as Person:<surname>, <forenames>
# 
# this function was originally introduced to standardize the name format of the WA site pages
# that were arranged initially as <forename> <familyname> format.
# 
def reformat(page_name):
    profile = False
    name_loc = re.search("Person:",page_name) # check for "Person:" or "Profile:"
    if not name_loc:
      name_loc = re.search("Profile:",page_name)
      profile = name_loc
    if name_loc:                            # this is a "Person:" page name - don't do anything otherwise
        page_name = page_name[name_loc.end():]  # strip "Person:"
        # Find the text in brackets provided by perplaxity.ai
        bracket_text = re.findall(r'\((.*?)\)', page_name)
        # Remove the text in brackets from the original string
        page_name = re.sub(r'\((.*?)\)', '', page_name)
        
        page_name = re.sub("_"," ",page_name)   # substitute _ with whitespace
        page_name = page_name.rstrip("\n")  # remove \n character if present
        page_name = page_name.lstrip(" ")   # remove whitespace from front
        page_name = page_name.rstrip(" ")   # and from end too
        if not re.search(",", page_name):       # no comma found in name - need to reformat
            forenames = ""                      # initialize
            family_name = page_name             # in case of single name only
            while space_loc := re.search(r'\s', page_name):                 # search for next whitespace char
                forenames = forenames + ' ' + page_name[:space_loc.start()] # add this name to fornames
                page_name = page_name[space_loc.end():]                     # trim text
                family_name = page_name                                     # rest will be family name unless another space found
            if profile:
              page_name = "Profile:" + family_name + ',' + forenames       # reformat the page title
            else:
              page_name = "Person:" + family_name + ',' + forenames       # reformat the page title
            
            if bracket_text:
                page_name = page_name + ' (' + ', '.join(bracket_text) + ')'  # replace bracket string at end
           
        else:                                                           # comma found - name format already in correct format               
          if profile:
            page_name = "Profile:" + page_name
          else:
            page_name = "Person:" + page_name
            
    return(page_name)
    
    
#=====================================================================================================
#
# function to to check for bad internal links by referring to the reference pages list
# 
def check_links(pagetext, pagetitle, download, pages_list):
   bad_links = []
   if pagetitle == "Sitemap":
     return bad_links
   content = pagetext
   links = re.findall(r'\[\[(.+)\]\]',content)
   for link in links:
     if link[0:6] == "Media:":
       i=1
     elif link[0:8] == "Special:":
       i=1
     elif link[0:5] == "File:":
       i=1
     elif link[0:9] == "Category:":
       i=1
     else:
       # internal page link
       bar_loc = re.search(r'\|',link)
       plink = link
       if bar_loc:
         plink = link[:bar_loc.start()]
       plink = plink.rstrip(" ")
       plink = plink.rstrip(" ")
       plink = plink.rstrip(" ")
       plink = re.sub("_"," ",plink)
       if plink not in pages_list:
         bad_links += [plink + "|" + pagetitle]
         outfile.write("bad link " + plink + " in " + pagetitle + "\n")
         
   if bad_links == []:
     outfile.write(pagetitle + " has no bad links\n")
     
   return bad_links
     
#=====================================================================================
#
# Function to parse text with separator character - ignoring trailing and leading spaces
# Note that special characters such as a vertical bar must be specified with the 
# backslash, for example r'\|'
#
# This function was introduced after much of the code was written in a rather
# clumsy style. Several sections of code could be rearranged to use this function
# and appear much cleaner.
#
def separate_text(sep, text):

  items = []
  found = True
  m_pt = 0
  en_pt = 0
  n_items = 0
  while found:
    #print(text[m_pt:])
    sep_p = re.search(sep,text[m_pt:])
    if sep_p == None:
      found = False
      item = text[m_pt:]
    else:
      n_items += 1
      st_pt = sep_p.start()
      en_pt = sep_p.end()
      item = text[m_pt:st_pt + m_pt]
    
      
    #print(str(m_pt), str(st_pt), str(en_pt), item)
    if len(item) > 0:
      while item[0] == " ":
        item = item.lstrip(" ")
        if len(item) == 0:
          break
    if len(item) > 0:
      while item[len(item)-1] == " " and len(item) > 0:
        item = item.rstrip(" ")
        if len(item) == 0:
          break
    items += [item]    
    m_pt = en_pt + m_pt
    if n_items > 10:
      found = False

  return items

#====================================================================================================
#
# function to identify image and media file references and download files to local media folder 
#
def download_media(pagetext, pagetitle, download, media_file_list):
    
  pagetext = re.sub(r'\&lt\;PRE\&gt\;(.+?)\&lt\;\/PRE\&gt\;', "", pagetext, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)  # remove <pre> .... </pre> text segments  
  pagetext = re.sub(r'\&lt\;NOWIKI\&gt\;(.+?)\&lt\;\/NOWIKI\&gt\;', "", pagetext, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)  # remove <nowiki> .... </nowiki> text segments
  # extract alphabetically sorted list of image file references in the XML text
  images = extract_image_files(pagetext)
  sort_images = sorted(images)
  missing_media_files = []
  #print(pagetext)
  
  # check to see if any of the referenced image files is not in the images folder
  for name in sort_images:
     name = unquote(name)
     name = replace_underscore(name)

     description = "used in " + pagetitle + "\n"
     if desc_write: 
       descfile = open(description_folder + name + ".txt", "a")            # append record to description file
       descfile.write(description)
       descfile.close()

     if name not in media_file_list:  # has not been downloaded yet
         missing_media_files += [name]
         media_file_list += [name]
         # create / append description file    
         
         if download:
           if name not in media_file_list:
              outfile.write(name)
              outfile.write(" is not in the media files folder\n")
              image_url = wiki_url + name
              
              # download as a stream to overcome output buffer limits at server
        
              response = requests.get(image_url, stream=True)
              image_file_location = download_path + name
        
              if response.status_code == 200:
                  with open(image_file_location, 'wb') as image_outfile:
                      for chunk in response.iter_content(chunk_size=8192):
                          image_outfile.write(chunk)
                  image_outfile.close()
                  outfile.write(image_url)
                  outfile.write(" downloaded successfully\n")
                  print(image_url," downloaded successfully")
                  outfile.write(name)
                  outfile.write(" saved\n")
              else:
                  outfile.write(image_url)
                  outfile.write(" could not be accessed (" + pagetitle + ")\n")
                  print(image_url," could not be accessed (" + pagetitle + ")")
     
  media = extract_media_files(pagetext)
  sort_media = sorted(media)
  
  # check to see if any of the referenced media files is not in the images folder
  for name in sort_media:
     name = replace_underscore(name)
     name = unquote(name)

     # create / append description file    
     description = "used in " + pagetitle + "\n"
     if desc_write: 
       descfile = open(description_folder + name + ".txt", "a")            # open description file to append
       descfile.write(description)
       descfile.close()


     if name not in media_file_list:
         missing_media_files += [name]   
         media_file_list += [name]
   
    
         if download:
           if name not in media_file_list:
              outfile.write(name)
              outfile.write(" is not in the images folder\n")
              media_url = wiki_url + name
              
              # download as a stream to overcome output buffer limits at server
        
              response = requests.get(media_url, stream=True)
              media_file_location = download_path + name
        
              if response.status_code == 200:
                  with open(media_file_location, 'wb') as media_outfile:
                      for chunk in response.iter_content(chunk_size=8192):
                          media_outfile.write(chunk)
                  media_outfile.close()
                  outfile.write(media_url)
                  outfile.write(" downloaded successfully\n")
                  print(media_url," downloaded successfully")
                  outfile.write(name)
                  outfile.write(" saved\n")
              else:
                  outfile.write(media_url)
                  outfile.write(" could not be accessed (" + pagetitle + ")\n")
                  print(media_url," could not be accessed (" + pagetitle + ")")
  
  outfile.write("\n")
  return media_file_list

#====================================================================================================
#
# Function to generate a list of categories sorted by order in the category_list file
#
def category_sort(cats, categories):
  #print(cats)
  
  c_list = []
  cnum = 0
  
  for cat in cats:
    cnum = 0
    if len(cat) == 1:
      c_list += [(0,cat)]
    else:  
      for category in categories:
        cat = re.sub("_"," ",cat)
        if str.lower(category) == str.lower(cat):
           c_list += [(cnum,cat)]
        cnum += 1   
 
  sort_list = sorted(c_list, key=lambda x: x[0])
  sorted_cat_string = ""
  for cat in sort_list:
    catstr = cat[1]
    sorted_cat_string = sorted_cat_string + catstr + "; "
  return(sorted_cat_string)


#====================================================================================================
#
# Function to look up reference page list
#
def lookup_ref_page(ref_pages_list, pagename):
  i = 0
  for page in ref_pages_list:
    if pagename == page:
      return i
    i += 1
  return -1      


#===================================================================================================
#
#
# Main XML file processing code
#


# Read XML file into string "filetext"

with open(xml_data_file, 'rb') as file:
    content = file.read()
    filetext = content.decode('utf-8')
file.close()

# Read categories list
category_list = read_list_file(categories_file_name)
# Read media file list
media_file_list = read_list_file(media_file_list_name)
# Read pages reference file
ref_pages_list = read_list_file(page_ref_file_name)
ref_page_name_list = []
summary_list = []
for page in ref_pages_list:
  items = separate_text(r'\|',page)
  if len(items) > 1 and items[0] != "":
    ref_page_name_list += [items[1]]      # page name
    summary_list += [items[len(items)-1]] # summary is last item
    #print(items[1],items[len(items)-1])
  else:
    ref_page_name_list += ["nodata"]
    summary_list += ["nodata"]
 
# Process the file, page by page

remaining_file_text = filetext
pages_list = extract_page_names(filetext)
new_file_text = ""
page_data = True
ppages = []
opages = []
plpages = []
mpages = []
namespace = ""
categories = ""
name = ""
n_ppages = 0
n_opages = 0
n_mpages = 0
n_plpages = 0
bad_links = []
numpage = 0

while page_data:
  print("Page ",str(numpage),"\r",end='')
  numpage += 1
  bad_link_list = []
  page_match = re.search(r'<page>(.+?)</page>', remaining_file_text, re.DOTALL)     # find next page
  if page_match:                                       
    page_data = True                                                           # there is a page
    page_text = page_match.group(1)
    remainder = remaining_file_text[:page_match.start()]                       # string up to just before start of <page>
    remaining_file_text = remaining_file_text[page_match.end():]               # discard this text from remaining text

    title_match = re.search(r'<title>(.+?)</title>',page_text)                  # title defined?
    if title_match:
      pagetitle = title_match.group(1)                                         # retrieve page title
      ipage = lookup_ref_page(ref_page_name_list,pagetitle)                    # find in reference pages list
      newpagetitle = reformat(pagetitle)                                       # reformat
        
      # remove any instances of a space character from between "File:" and filename in XML text, also for media references   
      page_text = re.sub('File: ', 'File:', page_text, flags=re.IGNORECASE)
      new_page_text = re.sub('Media: ', 'Media:', page_text, flags=re.IGNORECASE)
  
      # standardise case for instances of "File:"
      page_text = re.sub('File:', 'File:', page_text, flags=re.IGNORECASE)
      page_text = re.sub('Media:', 'Media:', page_text, flags=re.IGNORECASE)
      page_text = re.sub('ProFile:', 'Profile:', page_text, flags=re.IGNORECASE)  # restore instances of "Profile"!
  
      #  download media files not already available       
      media_file_list = download_media(page_text, newpagetitle, download, media_file_list)
 
      # log file
      outfile.write("Processing page " + pagetitle + "\n")

      # extract info from page depending on namespace value
      namespace_match = re.search(r'<ns>(.+?)</ns>',page_text)
      timestamp_match = re.search(r'<timestamp>(.+?)</timestamp>',page_text)
      redirect_match = re.search(r'#REDIRECT', page_text)
      
      if namespace_match:
        namespace = namespace_match.group(1)
      else:
        outfile.write("No namespace found\n")
      if timestamp_match:
        timestamp = timestamp_match.group(1)
      else:
        timestamp = "none"
      retain_page = False
      
      # look for person pages
      if namespace == "3000" or namespace == "3002":
        person_match = re.search('Person:',newpagetitle)  # ensure family name first
        if not person_match:
          person_match = re.search('Profile:',newpagetitle)  # ensure family name first
        if person_match:
          name = newpagetitle[person_match.end():]
          retain_page = True
          n_ppages += 1
        else:
          outfile.write("No Person: or Profile: text found\n")
          name = "--"
        
        # look for categories
        remaining_page_text = page_text
        categories = []
        page_wiki_opening = re.search(r"<text.*?>", remaining_page_text, re.DOTALL) # search for <text ....> string
        if page_wiki_opening:
          wikitext = page_text[page_wiki_opening.end():]
          remaining_page_text = re.sub(r'\&lt\;pre\&gt\;(.+?)\&lt\;\/pre\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <pre> .... </pre> text segments
          remaining_page_text = re.sub(r'\&lt\;nowiki\&gt\;(.+?)\&lt\;\/nowiki\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <nowiki> .... </nowiki> text segments
          #print(remaining_page_text)
          remaining_page_text = wikitext
          life_span = lifespan(remaining_page_text)
          category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
          while category_match:
            remaining_page_text = remaining_page_text[category_match.end():]
            categories += [category_match.group(1)]
            category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
        
          cat_string = ""
          if len(categories) > 0:
            cat_string = category_sort(categories, category_list)

          bad_link_list = check_links(wikitext, pagetitle, download, pages_list)

        else:
          chars = len(page_text)
          if chars > 800:
             out_text = page_text[0:800]
          else:
             out_text = page_text
          outfile.write("No text opening found:" + out_text + "\n\n")
        
        if ipage >= 0:
          summ = summary_list[ipage]
        else:
          summ = "None"
        page_entry = (newpagetitle + "|" + pagetitle + "|" + cat_string + "|" + timestamp_mon_year(timestamp) + "|" + life_span + "|" + summ )
        ppages += [page_entry]
        outfile.write(page_entry + "\n")
      
      elif namespace == "3008":   # organization pages
      
        org_match = re.search('Organisation:',pagetitle)  
        if org_match:
          oname = newpagetitle[org_match.end():]
          retain_page = True
          n_opages += 1
        else:
          outfile.write("No Organisation: text found\n")
          oname = "--"
        
        # look for categories
        remaining_page_text = page_text
        remaining_page_text = re.sub(r'\&lt\;pre\&gt\;(.+?)\&lt\;\/pre\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <pre> .... </pre> text segments
        remaining_page_text = re.sub(r'\&lt\;nowiki\&gt\;(.+?)\&lt\;\/nowiki\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <nowiki> .... </nowiki> text segments
        #print(remaining_page_text)
        life_span = lifespan(remaining_page_text)
        categories = []
        page_wiki_opening = re.search(r"<text.*?>", remaining_page_text, re.DOTALL) # search for <text ....> string
        if page_wiki_opening:
          wikitext = page_text[page_wiki_opening.end():]
          remaining_page_text = wikitext
          category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
          while category_match:
            remaining_page_text = remaining_page_text[category_match.end():]
            categories += [category_match.group(1)]
            category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
        
          cat_string = ""
          if len(categories) > 0:
            cat_string = category_sort(categories, category_list)
          
          bad_link_list = check_links(wikitext, pagetitle, download, pages_list)
        
        
        
        else:
          chars = len(page_text)
          if chars > 800:
             out_text = page_text[0:800]
          else:
             out_text = page_text
          outfile.write("No text opening found:" + out_text + "\n\n")

        if ipage >= 0:
          summ = summary_list[ipage]
        else:
          summ = "None"  
        page_entry = newpagetitle + "|" + pagetitle + "|" + cat_string + "|" + timestamp_mon_year(timestamp) + "|" + life_span + "|" + summ 
        opages += [page_entry]
        outfile.write(page_entry + "\n")
      
      elif namespace == "3004":   # place pages
      
        place_match = re.search('Place:',pagetitle)  
        if place_match:
          plname = newpagetitle[place_match.end():]
          retain_page = True
          n_plpages += 1
        else:
          outfile.write("No Place: text found\n")
          plname = "--"
        
        # look for categories
        remaining_page_text = page_text
        categories = []
        page_wiki_opening = re.search(r"<text.*?>", remaining_page_text, re.DOTALL) # search for <text ....> string
        if page_wiki_opening:
          remaining_page_text = page_text[page_wiki_opening.end():]
          remaining_page_text = re.sub(r'\&lt\;pre\&gt\;(.+?)\&lt\;\/pre\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <pre> .... </pre> text segments
          remaining_page_text = re.sub(r'\&lt\;nowiki\&gt\;(.+?)\&lt\;\/nowiki\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <nowiki> .... </nowiki> text segments
          life_span = lifespan(remaining_page_text)

          #print(remaining_page_text)
          wikitext = remaining_page_text
          category_match = re.search(r'\[\[Category:(.+?)]\]', remaining_page_text, flags=re.IGNORECASE)
          while category_match:
            remaining_page_text = remaining_page_text[category_match.end():]
            categories += [category_match.group(1)]
            category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
        
          cat_string = ""
          if len(categories) > 0:
            cat_string = category_sort(categories, category_list)
          
          bad_link_list = check_links(wikitext, pagetitle, download, pages_list)


        else:
          chars = len(page_text)
          if chars > 800:
             out_text = page_text[0:800]
          else:
             out_text = page_text
          outfile.write("No text opening found:" + out_text + "\n\n")

        if ipage >= 0:
          summ = summary_list[ipage]
        else:
          summ = "None"  
        page_entry = newpagetitle + "|" + pagetitle + "|" + cat_string + "|" + timestamp_mon_year(timestamp) + "|" + life_span + "|" + summ
        plpages += [page_entry]
        outfile.write(page_entry + "\n")
      
      elif namespace == "0":   # main pages
        retain_page = False
        if redirect_match:
          retain_page = False
        elif re.search('Css:',pagetitle):
          retain_page = False
        elif re.search('Forum:',pagetitle):  
          retain_page = False
        elif re.search('Home:',pagetitle):  
          retain_page = False
        elif re.search('Includepopup:',pagetitle):  
          retain_page = False
        elif re.search('Includes:',pagetitle):  
          retain_page = False
        elif re.search('Legal:',pagetitle):  
          retain_page = False
        elif re.search('Main:',pagetitle):  
          retain_page = False
        elif re.search('Maps Home',pagetitle):  
          retain_page = False
        elif re.search('Popuptes:',pagetitle):  
          retain_page = False
        elif re.search('Search:',pagetitle):  
          retain_page = False
        elif re.search('Sitema:',pagetitle):  
          retain_page = False
        elif re.search('System:',pagetitle):  
          retain_page = False
        elif re.search('Tes:',pagetitle):  
          retain_page = False
        elif re.search('Events:',pagetitle):  
          retain_page = False
        elif re.search('Help:',pagetitle):  
          retain_page = False
        elif re.search('Sitemap',pagetitle):
          retain_page = False
        elif re.search('Edit Cheat Sheet',pagetitle):
          retain_page = False
        elif re.search('Broken links',pagetitle):
          retain_page = False
        else:  
          retain_page = True
          n_mpages += 1
        
        # look for categories
        remaining_page_text = page_text
        categories = []
        page_wiki_opening = re.search(r"<text.*?>", remaining_page_text, re.DOTALL) # search for <text ....> string
        if page_wiki_opening:
          remaining_page_text = page_text[page_wiki_opening.end():]
          remaining_page_text = re.sub(r'\&lt\;pre\&gt\;(.+?)\&lt\;\/pre\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <pre> .... </pre> text segments
          remaining_page_text = re.sub(r'\&lt\;nowiki\&gt\;(.+?)\&lt\;\/nowiki\&gt\;', '', remaining_page_text, flags = re.DOTALL | re.IGNORECASE | re.MULTILINE)  # remove <nowiki> .... </nowiki> text segments
          #print(remaining_page_text)
          wikitext = remaining_page_text
          life_span = ""
          
          category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
          while category_match:
            remaining_page_text = remaining_page_text[category_match.end():]
            categories += [category_match.group(1)]
            category_match = re.search(r'\[\[Category:(.+?)\]\]', remaining_page_text, flags=re.IGNORECASE)
        
          cat_string = ""
          if len(categories) > 0:
            cat_string = category_sort(categories, category_list)

          bad_link_list = check_links(wikitext, pagetitle, download, pages_list)

        else:
          chars = len(page_text)
          if chars > 800:
             out_text = page_text[0:800]
          else:
             out_text = page_text
          outfile.write("No text opening found:" + out_text + "\n\n")

        if ipage >= 0:
          summ = summary_list[ipage]
        else:
          summ = "None"  
        page_entry = newpagetitle + "|" + pagetitle + "|" + cat_string + "|" + timestamp_mon_year(timestamp) + "|" + life_span + "|" + summ
        if retain_page:
          mpages += [page_entry]
        outfile.write(page_entry + "\n")
      
        for link in bad_link_list:
          bad_links += [link] 
        bad_link_list = []
  
       

    else: # no title match
      outfile.write("Page without title: suspect error\n")
      
    
    
  else: # no page match
    page_data = False



#
# Sort and scan lists for letter breaks - where first letter changes - for top of page index bar
# noting that ppages list has "profile:" and "person:" entries
#

temp_list=[]
for page in ppages:
  items = separate_text(r'\|',page)
  if items[0] != "":  # in case of blank entries
    names = separate_text(":",items[0])
    temp_list += [names[1] + "|" + page]  # extract full name and add to start of page entry for sort purposes
temp_list = sorted(temp_list)

ppages = []
for page in temp_list:
  items = separate_text(r'\|',page)
  page_entry = items[1] + "|" + items[2] + "|" + items[3] + "|" + items[4] + "|" + items[5] + "|" + items[6]
  ppages += [page_entry]

# rest can easily be sorted as they are

opages = sorted(opages)
plpages = sorted(plpages)
mpages = sorted(mpages)

previous_letter = ""
person_breaks = []
breaks = []
npage = 0
for page in ppages:
  npage += 1
  text = re.sub("Person:","",page)
  text = re.sub("Profile:","",page)
  text = re.sub("Place:","",text)
  text = re.sub("Organisation:","",text)
  first_letter = text[0]
  if first_letter != previous_letter:
    person_breaks += [first_letter]
    breaks += [npage]
    previous_letter = first_letter


# Generate new reference pages list file
new_pages_file = open(wkg_folder + new_pages_file_name,"w",encoding="utf-8")
for page in mpages:
  new_pages_file.write(page + "\n")
for page in ppages:
  new_pages_file.write(page + "\n")
for page in opages:
  new_pages_file.write(page + "\n")
for page in plpages:
  new_pages_file.write(page + "\n")
new_pages_file.close()

# Generate wikitable from pages lists
# Open wiki table and processed pages file - write
wiki_tab = open(wkg_folder + wiki_table_file, 'w',encoding="utf-8")
csv_file = open(wkg_folder + csv_file_name, 'w', encoding="utf-8")

wiki_tab.write("==Sitemap from " + xml_data_file + "==\n\n")
wiki_tab.write("| [[#Biographies|Biographies]] | [[#Organisations|List of Organisations]]  |  [[#Places|List of Places]] |\n\n")



# main pages - tabbed by states

wiki_tab.write("\n\n\n==Top Level Pages by State==\n\n")
csv_file.write("\n\n==Top Level Pages by State=\n\n")
states = ['National','Australian Capital Territory','New South Wales','Northern Territory','Queensland','South Australia','Victoria','Tasmania','Western Australia']

#open tabs
wiki_tab.write("<tabs>\n")

for state in states:
  wiki_tab.write("<tab name=\"" + state + "\">\n")

  wiki_tab.write("{| class = wikitable style=color:blue;")
  wiki_tab.write(" background-color:##ffcfcf; callpadding=5; width=100% \n")
  wiki_tab.write("\n! page !! summary [categories] !! timestamp \n")  # table header
  csv_file.write("\n\n" + state + "\t categories \t timestamp \t\n\n")

  for page in mpages:
    items = separate_text(r'\|',page)
    if len(items) < 6:
      outfile.write("Less than 6 items in page entry: " + str(items) + page + "\n")
    else:
      mname = items[0]
      pagetitle = items[1]
      cat = items[2]
      if cat != "":
        catl = "[" + cat + "]"
      else:
        catl = ""  
      ts = items[3]
      lifespan = items[4]
      summ = items[5]
      if summ == "None":
        summ = ""    
    
    state_loc = re.search(state,cat)  # if state found in categories
    show = False
    if state_loc:
      show = True
    else:                             # otherwise if none of the states is listed in categories...list under No State
      if state == 'National':
        show = True
        for state_i in states:
          state_loc1 = re.search(state_i,cat)
          if state_loc1:
            show = False
            
    #print("cat:",state,state_loc,show,state_loc1,cat)
    if show:
   
      # add entry to wiki-table.txt file - names have to be in double quotes for Excel to ignore commas in names
      wiki_tab.write("|-\n| " + "[" + site_URL + re.sub(" ","_",pagetitle) + " " + mname + " ] ||")
      wiki_tab.write(" <small>" + summ + "<small>" + catl + "</small></small>||" + ts + "\n")
      csv_file.write(pagetitle + "\t" + mname + "\t" + cat + "\t" + ts + "\n")
      outfile.write("page:" + page + "|" + mname + "|" + pagetitle + "|" + cat + "|" + ts + "\n")

  wiki_tab.write("|}\n\n") # end table  
  wiki_tab.write("</tab>\n")

wiki_tab.write("</tabs>\n")





# People Pages
wiki_tab.write("==Biographies==\n\n")
csv_file.write("==Biographies==\n\n")
top_links = False

previous_letter = ""
page_breaks = []
breaks = []
npage = 0
for page in ppages:
  npage += 1
  text = re.sub("Person:","",page)
  text = re.sub("Place:","",text)
  text = re.sub("Profile:","",text)
  text = re.sub("Organisation:","",text)
  first_letter = text[0]
  if first_letter != previous_letter:
    page_breaks += [first_letter]
    breaks += [npage]
    previous_letter = first_letter


if top_links:
  # write line of letter group links  [[#Names A| A ]] | [[#Names B| B ]]
  first = True
  for letter in breaks:
    if not first:
      wiki_tab.write("|")
    wiki_tab.write("[[#" + letter + " Names|" + letter + "]]")
    first = False

  wiki_tab.write("\n\n")
else:
  #open tabs
  wiki_tab.write("<tabs>\n")
  


first = True
npage = 0
for page in ppages:

  npage += 1

  items = separate_text(r'\|',page)
  if len(items) < 6:
    outfile.write("Less than 6 items in page entry: " + str(items) + page + "\n")
  else:
    name = items[0]
    pagetitle = items[1]
    cat = items[2]
    if cat != "":
      catl = "[" + cat + "]"
    else:
      catl = ""  
    ts = items[3]
    lifespan = items[4]
    summ = items[5]
    if summ == "None":
      summ = ""    

    
  text = re.sub("Person:","",name)
  text = re.sub("Profile:","",text)
  text = re.sub("Place:","",text)
  text = re.sub("Organisation:","",text)

  outfile.write("page:" + page + "|==|" + text + "|" + pagetitle + "|" + cat + "|" + ts + "\n")

  if npage in breaks:
    if not first:
      wiki_tab.write("|}\n\n") # end table
      if not top_links:
        wiki_tab.write("</tab>\n")
        
    first = False  
    if top_links:
      wiki_tab.write("==" + name[0] + " Names==\n\n")  # write heading text
    else:
      wiki_tab.write("<tab name=\"" + text[0] + "\">\n")
        
    wiki_tab.write("{| class = wikitable style=color:black;")
    wiki_tab.write(" background-color:##cfcfcf; callpadding=5; width=100% \n")
    wiki_tab.write("\n! name !! life-span !! summay [categories] !! timestamp \n")  # table header
  
  # add entry to wiki-table.txt file - names have to be in double quotes for Excel to ignore commas in names
  wiki_tab.write("|-\n| " + "[" + site_URL + re.sub(" ","_",pagetitle) + " " + text + " ] || " + lifespan + " ||")
  wiki_tab.write(" <small>" + summ + "<small>" + catl + "</small></small>||" + ts + "\n")
  csv_file.write(pagetitle + "\t" + text + " " + lifespan + "\t" + cat + "\t" + ts + "\n")

if top_links:
  wiki_tab.write("|}\n\n") # end table  
else:
  wiki_tab.write("|}\n\n") # end table  
  wiki_tab.write("</tab>\n</tabs>\n")
  
  
# organisations table - currently not enough for tabs

wiki_tab.write("\n\n\n==Organisations==\n\n")
csv_file.write("\n\n==Organisations==\n\n")

breaks = [1] # disable tabs
first = True
npage = 0
top_links = True

for page in opages:
  npage += 1
  items = separate_text(r'\|',page)
  if len(items) < 6:
    outfile.write("Less than 6 items in page entry: " + str(items) + page + "\n")

  else:  
    name = items[0]
    pagetitle = items[1]
    cat = items[2]
    if cat != "":
      catl = "[" + cat + "]"
    else:
      catl = ""  
    ts = items[3]
    lifespan = items[4]
    summ = items[5]
    if summ == "None":
      summ = ""    


  text = re.sub("Person:","",name)
  text = re.sub("Place:","",text)
  text = re.sub("Organisation:","",text)

  outfile.write("page:" + page + "|==|" + text + "|" + pagetitle + "|" + cat + "|" + ts + "\n")
  if npage in breaks:
    if not first:
      wiki_tab.write("|}\n\n") # end table
      if not top_links:
        wiki_tab.write("</tab>\n")
        
    first = False  
    if top_links:
      #wiki_tab.write("==" + name[0] + " Names==\n\n")  # write heading text
      wiki_tab.write("\n")
    else:
      wiki_tab.write("<tab name=\"" + name[0] + "\">\n")
        
    wiki_tab.write("{| class = wikitable style=color:blue;")
    wiki_tab.write(" background-color:##ffcfcf; callpadding=5; width=100% \n")
    wiki_tab.write("\n! oganisation !! lifespan !! summary [categories] !! timestamp \n")  # table header
  
  # add entry to wiki-table.txt file - names have to be in double quotes for Excel to ignore commas in names
  wiki_tab.write("|-\n| " + "[" + site_URL + re.sub(" ","_",pagetitle) + " " + text + " ] ||")
  wiki_tab.write(lifespan + " || " + " <small>" + summ + "<small>" + catl + "</small></small>||" + ts + "\n")
  csv_file.write(pagetitle + "\t" + text + " " + lifespan + "\t" + cat + "\t" + ts + "\n")

if top_links:
  wiki_tab.write("|}\n\n") # end table  
else:
  wiki_tab.write("|}\n\n") # end table  
  wiki_tab.write("</tab>\n</tabs>\n")



previous_letter = ""
page_breaks = []
breaks = []
npage = 0
for page in plpages:
  npage += 1
  text = re.sub("Person:","",page)
  text = re.sub("Place:","",text)
  text = re.sub("Profile:","",text)
  text = re.sub("Organisation:","",text)
  first_letter = text[0]
  if first_letter != previous_letter:
    page_breaks += [first_letter]
    breaks += [npage]
    previous_letter = first_letter
  

# places table - enough for tabs

wiki_tab.write("\n\n\n==Places==\n\n")
csv_file.write("\n\n==Places==\n\n")

top_links = False

if top_links:
  # write line of letter group links  [[#Names A| A ]] | [[#Names B| B ]]
  first = True
  for letter in page_breaks:
    if not first:
      wiki_tab.write("|")
    wiki_tab.write("[[#" + letter + " Names|" + letter + "]]")
    first = False

  wiki_tab.write("\n\n")
else:
  #open tabs
  wiki_tab.write("<tabs>\n")


first = True
npage = 0


for page in plpages:
  npage += 1
  items = separate_text(r'\|',page)
  if len(items) < 6:
    outfile.write("Less than 6 items in page entry: " + str(items) + page + "\n")
  else:
    plname = items[0]
    pagetitle = items[1]
    cat = items[2]
    if cat != "":
      catl = "[" + cat + "]"
    else:
      catl = ""  
    ts = items[3]
    lifespan = items[4]
    summ = items[5]
    if summ == "None":
      summ = ""    

  text = re.sub("Person:","",plname)
  text = re.sub("Place:","",text)
  text = re.sub("Profile:","",text)
  text = re.sub("Organisation:","",text)
  
  outfile.write("page:" + page + "|" + text + "|" + pagetitle + "|" + cat + "|" + ts + "\n")
  if npage in breaks:
    if not first:
      wiki_tab.write("|}\n\n") # end table
      if not top_links:
        wiki_tab.write("</tab>\n")
        
    first = False  
    if top_links:
      wiki_tab.write("==" + plname[0] + " Names==\n\n")  # write heading text
      wiki_tab.write("\n")
    else:
      wiki_tab.write("<tab name=\"" + text[0] + "\">\n")
        
    wiki_tab.write("{| class = wikitable style=color:green;")
    wiki_tab.write(" background-color:##cfffcf; callpadding=5; width=100% \n")
    wiki_tab.write("\n! place !! summary [categories] !! timestamp \n")  # table header
  
  # add entry to wiki-table.txt file - names have to be in double quotes for Excel to ignore commas in names
  wiki_tab.write("|-\n| " + "[" + site_URL + re.sub(" ","_",pagetitle) + " " + text + " ] ||")
  wiki_tab.write(lifespan + " || " + " <small>" + summ + "<small>" + catl + "</small></small>||" + ts + "\n")
  csv_file.write(pagetitle + "\t" + text  + "\t" + cat + "\t" + ts + "\n")

if top_links:
  wiki_tab.write("|}\n\n") # end table  
else:
  wiki_tab.write("|}\n\n") # end table  
  wiki_tab.write("</tab>\n</tabs>\n")



#print(bad_links)
if len(bad_links) > 0:
  wiki_tab.write("\n\n\n==Possible broken links==\n\n")
  csv_file.write("\n\n==Possible broken links=\n\n")

  for link in bad_links:
    bar_loc = re.search(r'\|',link)
    plink = unquote(link[:bar_loc.start()])
    pagetitle = unquote(link[bar_loc.end():])
    wiki_tab.write("[[" + plink + "]] in page [[" + pagetitle + "]]<br>\n")
    csv_file.write(plink + "\t" + pagetitle + "\n")

print(str(n_ppages)," person (biography) pages\n")
print(str(n_opages)," organisation pages\n")
print(str(n_plpages)," place pages\n")
print(str(n_mpages)," top-level or unclassified pages\n")


wiki_tab.close()
csv_file.close()
outfile.close()


outfile = open("new_media_file_list.txt","w",encoding="UTF-8")
for file_name in media_file_list:
  outfile.write(file_name + "\n")
outfile.close()

outfile = open("bad_links_list.txt","w",encoding="UTF-8")
for link in bad_links:
  outfile.write(link + "\n")
outfile.close()

 